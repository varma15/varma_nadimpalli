#Read the data 

```{r}
rm(list = ls())

setwd("~/Downloads/spambase")
spamdata<-read.table("~/Downloads/spambase/spambase.data",header = T,sep=",")
write.csv(spamdata,file="spamdata.csv",row.names = F)

```
#missing values
```{r}
sum(is.na(spamdata))
spamdata$X1<-as.factor(spamdata$X1)
str(spamdata)

```
#Bar plots of the data.
```{r}
par(mfrow=c(1,1))
counts <- table(X1,X0.1)
barplot(counts, main="#Cylinders in each fuel_system type",
        col=rainbow(7),
        xlab="Fuel_system", ylab = "#cars",
        legend.text = TRUE,
        args.legend = list(x = "topright", bty = "n", 
                           cex = 0.6, ncol=1, xjust=1),beside = TRUE)attach(spamdata)
par(mfrow=c(1,2))#to show 2 charts at a time
par(mfrow=c(1,1))
barplot(table(spamdata$X1),main="number of spam and not spam")
barplot(table(spamdata$X0.1))
barplot(table(spamdata$X0))
barplot(table(spamdata$X0.64))
barplot(table(spamdata$X0.64.1))
barplot(table(spamdata$X0.32))
barplot(table(spamdata$X0.2))
barplot(table(spamdata$X0.3))
cor(spamdata)
spamdata1<-subset(spamdata,select=-c(X1))
library(corrplot)
corrplot(cor(spamdata1),method = "color")
plot(spamdata$X1)X1

```
#summary and structure of data
```{r}
#install.packages("fBasics")
library(fBasics)
x<-data.frame(basicStats(spamdata))
#write.csv(x,"descriptive statistics.csv",row.names = F)
```
#class variable into factor.
```{r}
spamdata$X1<-as.factor(spamdata$X1)
str(spamdata)
```
#split the data into test and train
```{r}
#install.packages("caret")
library(caret)
set.seed(555)
train_rows <- createDataPartition(spamdata$X1, p = 0.7, list = F)
train_data <- spamdata[train_rows, ]
test_data <- spamdata[-train_rows, ]
```
#standardise data
```{r}
train_datanew=subset(train_data,select=-c(X1))
test_datanew=subset(test_data,select=-c(X1))


std_method <- prcomp(train_datanew,center = T,scale. = T)

train_datanew <- predict(std_method, train_datanew)
train_datanew<-as.data.frame(train_datanew)
  
test_datanew <- predict(std_method, test_datanew)
test_datanew<-as.data.frame(test_datanew)
plot(std_method, type = "l")
summary(std_method)
train_datanew<-subset(train_datanew,select = c(PC1:PC7))
test_datanew<-subset(test_datanew,select=c(PC1:PC7))
X1<-train_data$X1
train_datanew1<-as.data.frame(cbind(train_datanew,X1))
X1<-test_data$X1
test_datanew1<-as.data.frame(cbind(test_datanew,X1))
```
#BEST FIT MODEL FOR CROSS VALIDATION
```{r}
fitControl <- trainControl(method = "cv",number = 5,savePredictions = 'final',classProbs = F)
```
#run logistic model
```{r}
attach(spamdata)
log_reg <- glm(X1~., data = train_datanew1, family = binomial,control = list(maxit=100))
summary(log_reg)
library(MASS)
#list of predictions using the pred function
prob_train <- predict(log_reg, type = "response")
prob_train
#Using the ROCR package create a "prediction()" object
library(ROCR)
library(gplots)
pred <- prediction(prob_train, train_datanew1$X1)
pred
#Extract performance measures (True Positive Rate and False Positive Rate) using the "performance()" function from the ROCR package
perf <- performance(pred, measure="tpr", x.measure="fpr")
perf
#Plot the ROC curve using the extracted performance measures (TPR and FPR)

#plot(perf, col=rainbow(10), colorize=T, print.cutoffs.at=seq(0,1,0.05))
# Extract the AUC score of the ROC curve and store it in a variable named "auc"
perf_auc <- performance(pred, measure="auc")

auc <- perf_auc@y.values[[1]]
print(auc)
## Predictions on test data
 #After choosing a cutoff value, predict the class labels on the test data using our model
prob_test <- predict(log_reg, test_datanew1, type = "response")

preds_test <- ifelse(prob_test > 0.4, "1", "0")
#confusion matrix
test_data_labs <-test_datanew1$X1

conf_matrix <- table(test_data_labs, preds_test)

print(conf_matrix)
#install.packages('e1071',dependencies = T)
library(e1071)
library(caret)
confusionMatrix(test_data_labs,preds_test,positive = "0")
```
#Neural network.
```{r}

library(mxnet)
train.x = data.matrix(train_datanew1[,-8])
train.y = as.numeric(as.character(train_datanew1[,8]))
test.x = data.matrix(test_datanew1[,-8])
test.y = as.numeric(as.character(test_datanew1[,8]))
mx.set.seed(98)
Sys.time() -> start
model_mlp <- mx.mlp(train.x, train.y, hidden_node=c(20,20), out_node=2,activation="relu", out_activation="softmax",num.round=20, array.batch.size=100, learning.rate=0.9, momentum=0.8,eval.metric=mx.metric.accuracy)
 Sys.time() -> end
 paste(end - start)

preds = predict(model_mlp, test.x)
preds=t(preds)
pred.label1<-ifelse(preds[,2]>0.55,1,0)
confusionMatrix(as.factor(pred.label1),as.factor(test.y),positive = "0")

```
#SVM
pred.label1<-ifelse(preds[,1]>0.5,1,0)
confusionMatrix(as.factor(pred.label1),as.factor(test.y),positive = "0")
```{r}
library(e1071)

model_svm <- svm(X1 ~ . , train_datanew1, kernel = "linear")

summary(model_svm)
preds_svmtrain<-predict(model_svm, train_datanew1)
preds_svm <- predict(model_svm, test_datanew1)
test_lab<-test_datanew1$X1
confusionMatrix(preds_svm, test_lab,positive = "0")

preds_train_svm <- predict(model_svm)
ctrl <- trainControl(method="repeatedcv",repeats = 1)
rpart.grid <- expand.grid(C=seq(10,100,10))
model_svmval<-train(X1~.,data=train_datanew1,method='svmLinear',trControl=ctrl,tuneLength=3,tuneGrid=rpart.grid)
#obj = tune.svm(X1 ~ . ,data=train_datanew1,cost=10:100,gamma=seq(0,3,0.1))
preds_svm_val <- predict(model_svmval, test_datanew1)

confusionMatrix(preds_svm_val, test_datanew1$X1,positive = "0")
summary(model_svmval)
model_svmval
```
#SVM tandoth
```{r}
library(e1071)
library(kernlab)
model_svm_th <- ksvm(X1 ~ . ,train_datanew1, kernel = "tanhdot")
preds_svm_th <- predict(model_svm_th, test_datanew1)

confusionMatrix(preds_svm_th, test_datanew1$X1)
svmtanh_aucplot<-plot.roc(as.numeric(test_datanew1$X1),as.numeric(preds_svm_th),lwd=2,type="b",print.auc=T,col="blue",main="svmtanh")

preds_train_svm_th <- predict(model_svm_th)

```
#Random Forest
```{r}

library(randomForest)
model_rf <- randomForest(X1 ~ . , train_datanew1,ntrees=1500)
preds_rf <- predict(model_rf, test_datanew1)
preds_rftrain <- predict(model_rf, train_datanew1,type="prob")
preds_rftrain2<-ifelse(preds_rftrain[,1]>preds_rftrain[,2],preds_rftrain[,1],preds_rftrain[,2])
preds_rftest <- data.frame(predict(model_rf, test_datanew1,type="prob"))
preds_rftest2<-ifelse(preds_rftest[,1]>preds_rftest[,2],preds_rftest[,1],preds_rftest[,2])
preds_trainrf<-predict(model_rf, train_datanew1)
confusionMatrix(preds_rf, test_datanew1$X1)

preds_train_rf <- predict(model_rf)
ctrl <- trainControl(method="repeatedcv",repeats = 10)
rf.grid <- expand.grid(.mtry=7)
model_rfval<-train(X1~.,data=train_datanew1,method='rf',trControl=ctrl,tuneLength=3,tuneGrid=rf.grid)
preds_rftest1 <- predict(model_rfval, test_datanew1,type="prob")

preds_rfvaltest<-predict(model_rfval, test_datanew1)
confusionMatrix(preds_rfvaltest, test_datanew1$X1)
```
#KNN classifier
```{r}
ctrl <- trainControl(method="repeatedcv",repeats = 40)

#model_knncross<-train(X1~.,data=train_datanew1,method='knn',  tuneGrid=expand.grid(.k=1:90),trControl=ctrl,tuneLength=3)
#model_knncross
plot(model_knncross)
model_knn <- knn3(X1 ~ . , train_datanew1, k =7)
preds_k <- predict(model_knn, test_datanew1)
#preds_k1 <- predict(model_knncross, test_datanew1)
preds_k2<-ifelse(preds_k[, 1] > preds_k[, 2], preds_k[, 1], preds_k[, 2])
preds_knn <- ifelse(preds_k[, 1] > preds_k[, 2], 0, 1)
confusionMatrix(preds_knn, test_datanew1$X1)
#confusionMatrix(preds_k1, test_datanew1$X1)


```
#GBM 
```{r}
train_datanew1$X1 <- as.numeric(as.character(train_datanew1$X1))
str(train_datanew1)
test_datanew1$X1 <- as.numeric(as.character(test_datanew1$X1))
library(gbm)
model_gbm <-gbm(X1 ~ . , cv.folds = 20, interaction.depth = 2,shrinkage =0.005,distribution='bernoulli',data=train_datanew1, n.trees = 1500)
gbm.perf(model_gbm)
preds_g <- predict(model_gbm, type = 'response')
#install.packages("pROC")
library(pROC)
#install.packages("caret")
library(caret)
gbm_roc <- roc(train_datanew1$X1, preds_g)
cutoff_gbm <- coords(gbm_roc, "best", ret = "threshold")
preds_train_gbm <- ifelse(preds_g >= cutoff_gbm, 1, 0)

preds_test_g <- predict(model_gbm, test_datanew1, type = 'response')
preds_gbm <- ifelse(preds_test_g >= cutoff_gbm, 1, 0)
preds_test_g 
confusionMatrix(preds_gbm, test_datanew1$X1)

train_datanew1$X1 <- as.factor(as.character(train_datanew1$X1))

test_datanew1$X1 <- as.factor(as.character(test_datanew1$X1))

```
#Decision trees with cross validation
```{r}
library(caret)
library(rpart)
# grow tree 
ctrl <- trainControl(method="repeatedcv",repeats = 40)
rpart.grid <- expand.grid(.cp=seq(0.01,.2,.01))
model_rpart<-train(X1~.,data=train_datanew1,method='rpart',trControl=ctrl,tuneLength=3,tuneGrid=rpart.grid)
#fit <- rpart(X1 ~ ., data = train_datanew1,method="class")
#preds_r <- predict(fit, type = 'prob')
preds_r1<-predict(model_rpart,test_datanew1,type = 'prob')
preds_r2<-ifelse(preds_r1[,1]>=preds_r1[,2],preds_r1[,1],preds_r1[,2])

#summary(fit)
#Predict Output 
#predicted= predict(fit,test_datanew1)
#preds_train_dec <- ifelse(predicted[,2] >= .5, 1, 0)
preds_train_dec1<-ifelse(preds_r1[,1]>=preds_r1[,2],0,1)

#confusionMatrix(preds_train_dec, test_data_datanew1$X1)
confusionMatrix(preds_train_dec1, test_datanew1$X1)
model_rpart

```
#TAKING AVERAGE OF PREDICTIONS.
```{r}

test_datanew12<-data.frame((preds_rftest+preds_k)/3)
test_datanew13<-ifelse(test_datanew12[,1]>test_datanew12[,2],'0','1')
confusionMatrix(test_datanew13, test_datanew1$X1)


```
#TAKING MAJORITY VOTING OF PREICTIONS.
```{r}
test_datanew_majority<-ifelse(preds_gbm=='1' & preds_test=='1','1',ifelse(preds_gbm=='1' & preds_rf=='1','1','0'))

confusionMatrix(test_datanew_majority, test_datanew1$X1)

```
#TAKING WEIGHTED AVERAGE OF PREDICTIONS.
```{r}
test_dataaverage<-data.frame((preds_rftest*.5+preds_k*.25+preds_r1*.25))
test_datanewaverage1<-ifelse(test_dataaverage[,1]>0.5,'0','1')

confusionMatrix(test_datanewaverage1, test_datanew1$X1)

```
#STACKING WITH GBM AS TOP LAYER MODEL
```{r}
predictors_top<-cbind(prob_train,preds_g)
#preds_rftrain2
#GBM as top layer model 
model_gbm1<- 
train(predictors_top,train_datanew1[,8],method='gbm',trControl=fitControl,tuneLength=3)
predictorstest<-cbind(prob_test,preds_test_g)
#preds_rftest2
colnames(predictorstest)[colnames(predictorstest) == 'prob_test'] <- 'prob_train'
colnames(predictorstest)[colnames(predictorstest) == 'preds_test_g'] <- 'preds_g'
#colnames(predictorstest)[colnames(predictorstest) == 'preds_rftest2'] <- 'preds_rftrain2'
test_datanewstack<-predict(model_gbm1,predictorstest)
x123<-data.frame(test_datanewstack)
confusionMatrix(test_datanewstack, test_datanew1$X1)

```
#STACKING WITH GLM AS TOP LAYER MODEL
```{r}
predictors_top<-cbind(preds_rftrain2,preds_g)
#preds_rftrain2
#prob_train
model_glm2<-train(predictors_top,train_datanew1[,8],method='glm',trControl=fitControl,tuneLength=3)
predictorstest<-cbind(preds_rftest2,preds_test_g)
#preds_rftest2
#prob_test
#colnames(predictorstest)[colnames(predictorstest) == 'prob_test'] <- 'prob_train'
colnames(predictorstest)[colnames(predictorstest) == 'preds_test_g'] <- 'preds_g'
colnames(predictorstest)[colnames(predictorstest) == 'preds_rftest2'] <- 'preds_rftrain2'
test_datanewstack15<-predict(model_glm2,predictorstest)
confusionMatrix(test_datanewstack15, test_datanew1$X1)
```
Confusion matrix
```{r}
#logistic regression
confusionMatrix(test_data_labs,preds_test,positive = "0")
#Random forest
confusionMatrix(preds_rf, test_datanew1$X1)
confusionMatrix(preds_rfvaltest, test_datanew1$X1)
#KNN classifier
confusionMatrix(preds_knn, test_datanew1$X1)
#GBM
confusionMatrix(preds_gbm, test_datanew1$X1)
#Decision trees with cross validation
confusionMatrix(preds_train_dec1, test_datanew1$X1)
#TAKING AVERAGE OF PREDICTIONS.
confusionMatrix(test_datanew13, test_datanew1$X1)
#TAKING MAJORITY VOTING OF PREICTIONS.
confusionMatrix(test_datanew_majority, test_datanew1$X1)
#TAKING WEIGHTED AVERAGE OF PREDICTIONS.
confusionMatrix(test_datanewaverage1, test_datanew1$X1)
#STACKING WITH GBM AS TOP LAYER MODEL
confusionMatrix(test_datanewstack, test_datanew1$X1)
#STACKING WITH GLM AS TOP LAYER MODEL
confusionMatrix(test_datanewstack15, test_datanew1$X1)




```


