#Read the data 

```{r}
rm(list = ls())

setwd("~/Downloads/spambase")
spamdata<-read.table("~/Downloads/spambase/spambase.data",header = T,sep=",")
write.csv(spamdata,file="spamdata.csv",row.names = F)
barplot(table(spamdata$X1))
```
#missing values
```{r}
sum(is.na(spamdata))
spamdata$X1<-as.factor(spamdata$X1)
str(spamdata)

```
#Bar plots of the data.
```{r}
par(mfrow=c(1,1))
counts <- table(X1,X0.1)
par(mfrow=c(1,2))#to show 2 charts at a time
par(mfrow=c(1,1))
barplot(table(spamdata$X1),main="number of spam and not spam")
barplot(table(spamdata$X0.1))
barplot(table(spamdata$X0))
barplot(table(spamdata$X0.64))
barplot(table(spamdata$X0.64.1))
barplot(table(spamdata$X0.32))
barplot(table(spamdata$X0.2))
barplot(table(spamdata$X0.3))
boxplot(spamdata$X0~spamdata$X1)

cor(spamdata)
spamdata1<-subset(spamdata,select=-c(X1))
library(corrplot)
corrplot(cor(spamdata1),method = "color")
y<-cor(spamdata1)
plot(spamdata$X1)X1
library(stats)
plot(spamdata1$X0.23,spamdata1$X0.25)
#install.packages("MVN")
library(MVN)
library(mvoutlier)
result <- mvOutlier(spamdata, qqplot = TRUE, method = "quan")
library(caret)
uni.plot(spamdata2, symb=TRUE)
```
#summary and structure of data
```{r}
#install.packages("fBasics")
library(fBasics)
x<-data.frame(basicStats(spamdata))
#write.csv(x,"descriptive statistics.csv",row.names = F)
```
#class variable into factor.
```{r}
spamdata$X1<-as.factor(spamdata$X1)
str(spamdata)
spamdata<-subset(spamdata,select=-c(X0.25,X0.29))
```
#split the data into test and train
```{r}
#install.packages("caret")
library(caret)
set.seed(555)
train_rows <- createDataPartition(spamdata$X1, p = 0.7, list = F)
train_data <- spamdata[train_rows, ]
test_data <- spamdata[-train_rows, ]
```
#standardise data
```{r}
train_datanew=subset(train_data,select=-c(X1))
test_datanew=subset(test_data,select=-c(X1))


std_method <- preProcess(train_datanew,method = c("center","scale"))

train_datanew <- predict(std_method, train_datanew)
train_datanew<-as.data.frame(train_datanew)
  
test_datanew <- predict(std_method, test_datanew)
test_datanew<-as.data.frame(test_datanew)
plot(std_method, type = "l")
summary(std_method)
X1<-train_data$X1
train_datanew1<-as.data.frame(cbind(train_datanew,X1))
X1<-test_data$X1
test_datanew1<-as.data.frame(cbind(test_datanew,X1))
```
#BEST FIT MODEL FOR CROSS VALIDATION
```{r}
fitControl <- trainControl(method = "cv",number = 5,savePredictions = 'final',classProbs = F)
```
#run logistic model
```{r}
attach(spamdata)
log_reg <- glm(X1~., data = train_datanew1, family = binomial,control = list(maxit=100))
summary(log_reg)
library(MASS)
#list of predictions using the pred function
prob_train <- predict(log_reg, type = "response")
prob_train
#Using the ROCR package create a "prediction()" object
library(ROCR)
library(gplots)
pred <- prediction(prob_train, train_datanew1$X1)
pred
#Extract performance measures (True Positive Rate and False Positive Rate) using the "performance()" function from the ROCR package
perf <- performance(pred, measure="tpr", x.measure="fpr")
perf
#Plot the ROC curve using the extracted performance measures (TPR and FPR)

#plot(perf, col=rainbow(10), colorize=T, print.cutoffs.at=seq(0,1,0.05))
# Extract the AUC score of the ROC curve and store it in a variable named "auc"
perf_auc <- performance(pred, measure="auc")

auc <- perf_auc@y.values[[1]]
print(auc)
## Predictions on test data
 #After choosing a cutoff value, predict the class labels on the test data using our model
prob_test <- predict(log_reg, test_datanew1, type = "response")

preds_test <- ifelse(prob_test > 0.5, "1", "0")
#confusion matrix
test_data_labs <-test_datanew1$X1

conf_matrix <- table(test_data_labs, preds_test)

print(conf_matrix)
#install.packages('e1071',dependencies = T)
library(e1071)
library(caret)
confusionMatrix(test_data_labs,preds_test,positive = "0")
```
#Neural network.
```{r}

library(mxnet)
train.x = data.matrix(train_datanew1[,-56])
train.y = as.numeric(as.character(train_datanew1[,56]))
test.x = data.matrix(test_datanew1[,-56])
test.y = as.numeric(as.character(test_datanew1[,56]))
mx.set.seed(98)
Sys.time() -> start
model_mlp <- mx.mlp(train.x, train.y, hidden_node=c(20), out_node=2,activation="relu", out_activation="softmax",num.round=20, array.batch.size=50, learning.rate=0.05, momentum=0.5,eval.metric=mx.metric.accuracy)
 Sys.time() -> end
 paste(end - start)

preds = predict(model_mlp, test.x)
preds=t(preds)
pred.label1<-ifelse(preds[,2]>0.55,1,0)
confusionMatrix(as.factor(pred.label1),as.factor(test.y),positive = "0")

```
#SVM
```{r}
library(e1071)

model_svm <- svm(X1 ~ . , train_datanew1, kernel = "linear")

summary(model_svm)
preds_svmtrain<-predict(model_svm, train_datanew1)
preds_svm <- predict(model_svm, test_datanew1)
test_lab<-test_datanew1$X1
confusionMatrix(preds_svm, test_lab,positive = "0")

preds_train_svm <- predict(model_svm)
ctrl <- trainControl(method="repeatedcv",repeats = 1)
rpart.grid <- expand.grid(C=seq(10,100,10))
model_svmval<-train(X1~.,data=train_datanew1,method='svmLinear',trControl=ctrl,tuneLength=3,tuneGrid=rpart.grid)
#obj = tune.svm(X1 ~ . ,data=train_datanew1,cost=10:100,gamma=seq(0,3,0.1))
preds_svm_val <- predict(model_svmval, test_datanew1)

confusionMatrix(preds_svm_val, test_datanew1$X1,positive = "0")
summary(model_svmval)
model_svmval
rpart.grid1 <- expand.grid(C=30)
model_svmval1<-train(X1~.,data=train_datanew1,method='svmLinear',trControl=ctrl,tuneLength=3,tuneGrid=rpart.grid1)
```
#SVM tandoth
```{r}
library("pROC")
library(e1071)
library(kernlab)
model_svm_th <- ksvm(X1 ~ . ,train_datanew1, kernel = "tanhdot")
preds_svm_th <- predict(model_svm_th, test_datanew1)

confusionMatrix(preds_svm_th, test_datanew1$X1)
svmtanh_aucplot<-plot.roc(as.numeric(test_datanew1$X1),as.numeric(preds_svm_th),lwd=2,type="b",print.auc=T,col="blue",main="svmtanh")

preds_train_svm_th <- predict(model_svm_th)

```
#Random Forest
```{r}

library(randomForest)
model_rf <- randomForest(X1 ~ . , train_datanew1,ntrees=1500)
varImpPlot(model_rf)
preds_rf <- predict(model_rf, test_datanew1)
preds_rftrain <- predict(model_rf, train_datanew1,type="prob")
preds_rftrain2<-ifelse(preds_rftrain[,1]>preds_rftrain[,2],preds_rftrain[,1],preds_rftrain[,2])
preds_rftest <- data.frame(predict(model_rf, test_datanew1,type="prob"))
preds_rftest2<-ifelse(preds_rftest[,1]>preds_rftest[,2],preds_rftest[,1],preds_rftest[,2])
preds_trainrf<-predict(model_rf, train_datanew1)
confusionMatrix(preds_rf, test_datanew1$X1)

preds_train_rf <- predict(model_rf)
ctrl <- trainControl(method="repeatedcv",repeats = 10)
#rf.grid <- expand.grid(.mtry=7:20)
#model_rfval<-train(X1~.,data=train_datanew1,method='rf',trControl=ctrl,tuneLength=3,tuneGrid=rf.grid)
rf.grid <- expand.grid(.mtry=7)
model_rfval<-train(X1~.,data=train_datanew1,method='rf',trControl=ctrl,tuneLength=3,tuneGrid=rf.grid)
model_rfval
summary(model_rfval)
preds_rftest1 <- predict(model_rfval, test_datanew1,type="prob")

preds_rfvaltest<-predict(model_rfval, test_datanew1)
confusionMatrix(preds_rfvaltest, test_datanew1$X1)
plot(model_rfval)
```
#KNN classifier
```{r}
library(doMC)
registerDoMC(cores=4)
#ctrl <- trainControl(method="repeatedcv",repeats = 40)

#model_knncross<-train(X1~.,data=train_datanew1,method='knn',  tuneGrid=expand.grid(.k=1:9),trControl=fitControl,tuneLength=3)
#model_knncross
plot(model_knncross)
model_knn <- knn3(X1 ~ . , train_datanew1, k =7)
preds_train_knn<-predict(model_knn,train_datanew1)
preds_k <- predict(model_knn, test_datanew1)
#preds_k1 <- predict(model_knncross, test_datanew1)
preds_k2<-ifelse(preds_k[, 1] > preds_k[, 2], preds_k[, 1], preds_k[, 2])
preds_knn <- ifelse(preds_k[, 1] > preds_k[, 2], 0, 1)
confusionMatrix(preds_knn, test_datanew1$X1)
#confusionMatrix(preds_k1, test_datanew1$X1)


```
#GBM 
```{r}
train_datanew1$X1 <- as.numeric(as.character(train_datanew1$X1))
str(train_datanew1)
test_datanew1$X1 <- as.numeric(as.character(test_datanew1$X1))
library(gbm)
gbmGrid <-  expand.grid(interaction.depth = c(1, 3, 6, 9, 10),
                    n.trees = 1500, 
                    shrinkage = seq(.0005, .05,.005),
                    n.minobsinnode = 10)
#model_gbm1<-train(X1~.,data=train_datanew1,method='gbm',  tuneGrid=gbmGrid,trControl=fitControl,tuneLength=3)
model_gbm1
model_gbm <-gbm(X1 ~ . , cv.folds = 20, interaction.depth = 2,shrinkage =0.005,distribution='bernoulli',data=train_datanew1, n.trees = 1500)
model_gbm2 <-gbm(X1 ~ . , cv.folds = 20, interaction.depth = 10,shrinkage =0.0205,distribution='bernoulli',data=train_datanew1, n.trees = 1500)
preds_train_g2 <- predict(model_gbm2, train_datanew1)
preds_test_g2 <- predict(model_gbm2, test_datanew1)

preds_gbm <- ifelse(preds_test_g >= cutoff_gbm, 1, 0)

plot(model_gbm1)
gbm.perf(model_gbm)
gbm.perf(model_gbm2)

preds_g <- predict(model_gbm, type = 'response')
preds_g2 <- predict(model_gbm2,type = 'response')
confusionMatrix(preds_g1, test_datanew1$X1)


#install.packages("pROC")
library(pROC)
#install.packages("caret")
library(caret)
gbm_roc <- roc(train_datanew1$X1, preds_g)
gbm_roc1 <- roc(train_datanew1$X1, preds_g2)

cutoff_gbm <- coords(gbm_roc, "best", ret = "threshold")
cutoff_gbm1 <- coords(gbm_roc1, "best", ret = "threshold")

preds_train_gbm <- ifelse(preds_g >= cutoff_gbm, 1, 0)

preds_train_gbm1 <- ifelse(preds_g2 >= cutoff_gbm1, 1, 0)

preds_test_g <- predict(model_gbm, test_datanew1, type = 'response')
preds_test_g1 <- predict(model_gbm2, test_datanew1, type = 'response')
preds_gbm <- ifelse(preds_test_g >= cutoff_gbm, 1, 0)
preds_gbm1 <- ifelse(preds_test_g1 >= cutoff_gbm1, 1, 0)

preds_test_g 


confusionMatrix(preds_gbm, test_datanew1$X1)
confusionMatrix(preds_gbm1, test_datanew1$X1)


train_datanew1$X1 <- as.factor(as.character(train_datanew1$X1))

test_datanew1$X1 <- as.factor(as.character(test_datanew1$X1))

```
#Decision trees with cross validation
```{r}
library(caret)
library(rpart)
# grow tree 
ctrl <- trainControl(method="repeatedcv",repeats = 40)
rpart.grid <- expand.grid(.cp=seq(0.01,.2,.01))
model_rpart<-train(X1~.,data=train_datanew1,method='rpart',trControl=ctrl,tuneLength=3,tuneGrid=rpart.grid)
model_rpart
#fit <- rpart(X1 ~ ., data = train_datanew1,method="class")
#preds_r <- predict(fit, type = 'prob')
preds_rtrain<-predict(model_rpart,train_datanew1,type = 'prob')


preds_r1<-predict(model_rpart,test_datanew1,type = 'prob')
preds_r2<-ifelse(preds_r1[,1]>=preds_r1[,2],preds_r1[,1],preds_r1[,2])
preds_train_rpart<-ifelse(preds_rtrain[,1]>=preds_rtrain[,2],0,1)

#summary(fit)
#Predict Output 
#predicted= predict(fit,test_datanew1)
#preds_train_dec <- ifelse(predicted[,2] >= .5, 1, 0)
preds_train_dec1<-ifelse(preds_r1[,1]>=preds_r1[,2],0,1)

#confusionMatrix(preds_train_dec, test_data_datanew1$X1)
confusionMatrix(preds_train_dec1, test_datanew1$X1)
model_rpart

```
#TAKING AVERAGE OF PREDICTIONS.
```{r}

test_datanew12<-data.frame((preds_rftest+preds_k)/2)
test_datanew13<-ifelse(test_datanew12[,1]>test_datanew12[,2],'0','1')
confusionMatrix(test_datanew13, test_datanew1$X1)


```
#TAKING MAJORITY VOTING OF PREICTIONS.
```{r}
test_datanew_majority<-ifelse(preds_gbm=='1' & preds_test=='1','1',ifelse(preds_gbm=='1' & preds_rf=='1','1','0'))
test_datanew_majority1<-ifelse(preds_gbm=='1' & preds_rfvaltest=='1','1',ifelse(preds_gbm=='1' & preds_rf=='1','1','0'))
test_datanew_majority2<-ifelse(pred.label1=='1' & preds_rfvaltest=='1','1',ifelse(pred.label1=='1' & preds_rf=='1','1','0'))
test_datanew_majority3<-ifelse(pred.label1=='1' & preds_rfvaltest=='1','1',ifelse(pred.label1=='1' & test_datanew13=='1','1','0'))
test_datanew_majority3<-ifelse(pred.label1=='1' & preds_rfvaltest=='1','1',ifelse(pred.label1=='1' & test_datanew13=='1','1','0'))
test_datanew_majority4<-ifelse(pred.label1=='1' & preds_train_dec1=='1','1',ifelse(pred.label1=='1' & test_datanew13=='1','1','0'))
test_datanew_majority5<-ifelse(pred.label1=='1' & preds_gbm=='1','1',ifelse(pred.label1=='1' & test_datanew13=='1','1','0'))
test_datanew_majority6<-ifelse(pred.label1=='1' & test_datanewaverage1=='1','1',ifelse(pred.label1=='1' & test_datanew13=='1','1','0'))
test_datanew_majority7<-ifelse(preds_gbm=='1' & test_datanewstack=='1','1',ifelse(preds_gbm=='1' & preds_rf=='1','1','0'))
test_datanew_majority8<-ifelse(pred.label1=='1' & preds_rfvaltest=='1','1',ifelse(pred.label1=='1' & test_datanewstack15=='1','1','0'))
test_datanew_majority9<-ifelse(preds_gbm=='1' & preds_rfvaltest=='1','1',ifelse(preds_gbm=='1' & test_datanewstack15=='1','1','0'))
test_datanew_majority10<-ifelse(preds_gbm=='1' & test_datanew_majority9=='1','1',ifelse(preds_gbm=='1' & test_datanewstack15=='1','1','0'))
test_datanew_majority11<-ifelse(preds_rfvaltest=='1' & test_datanew_majority9=='1','1',ifelse(preds_rfvaltest=='1' & test_datanewstack15=='1','1','0'))
test_datanew_majority12<-ifelse(preds_rfvaltest=='1' & test_datanewstack=='1','1',ifelse(preds_rfvaltest=='1' & test_datanewstack15=='1','1','0'))
test_datanew_majority13<-ifelse(test_datanew_majority4=='1' & test_datanew_majority5=='1','1',ifelse(test_datanew_majority4=='1' & test_datanew_majority6=='1','1','0'))
test_datanew_majority14<-ifelse(test_datanew_majority12=='1' & test_datanew_majority13=='1','1',ifelse(test_datanew_majority12=='1' & test_datanew_majority11=='1','1','0'))
confusionMatrix(test_datanew_majority, test_datanew1$X1)#MAJORITY VOTING WITH GBM,GLM AND RANDOM FOREST
confusionMatrix(test_datanew_majority1, test_datanew1$X1)#MAJORITY VOTING WITH RANDOM FOREST,RANDOM FOREST WITH TUNING,GBM
confusionMatrix(test_datanew_majority2, test_datanew1$X1)
confusionMatrix(test_datanew_majority3, test_datanew1$X1)
confusionMatrix(test_datanew_majority4, test_datanew1$X1)
confusionMatrix(test_datanew_majority5, test_datanew1$X1)
confusionMatrix(test_datanew_majority6, test_datanew1$X1)
confusionMatrix(test_datanew_majority7, test_datanew1$X1)
confusionMatrix(test_datanew_majority8, test_datanew1$X1)
confusionMatrix(test_datanew_majority9, test_datanew1$X1)
confusionMatrix(test_datanew_majority10, test_datanew1$X1)
confusionMatrix(test_datanew_majority11, test_datanew1$X1)
confusionMatrix(test_datanew_majority12, test_datanew1$X1)
confusionMatrix(test_datanew_majority13, test_datanew1$X1)
confusionMatrix(test_datanew_majority14, test_datanew1$X1)



preds_train_dec1
```
#TAKING WEIGHTED AVERAGE OF PREDICTIONS.
```{r}
test_dataaverage<-data.frame((preds_rftest*.5+preds_k*.25+preds_r1*.25))
test_datanewaverage1<-ifelse(test_dataaverage[,1]>0.5,'0','1')

confusionMatrix(test_datanewaverage1, test_datanew1$X1)

```
#STACKING WITH GBM AS TOP LAYER MODEL
```{r}
train_preds_df <- data.frame(svm = preds_svmtrain,rf =preds_train_rf,knn = preds_train_knn,tree = preds_train_rpart, tree_bag = preds_train_tree_bag,gbm = preds_train_g2, X1 = train_datanew1$X1)
# Getting all the predictions from the validation data into a dataframe.
test_preds_df<- data.frame(svm=preds_svm,rf = preds_rfvaltest,knn = preds_k,tree =preds_train_dec1, tree_bag = preds_tree_bag,gbm=preds_test_g2,X1 = test_datanew1$X1)
stack_df <- rbind(train_preds_df)

stack_df$X1 <- as.factor(stack_df$X1)
numeric_st_df <- sapply(stack_df[, !(names(stack_df) %in% "X1")],function(x) as.numeric(as.character(x)))
pca_stack <- prcomp(numeric_st_df, scale = F)
predicted_stack <- as.data.frame(predict(pca_stack, numeric_st_df))[1:7]
stacked_df <- data.frame(predicted_stack, X1 = stack_df[, (names(stack_df) %in% "X1")])
#stacked_model <- svm(X1 ~ . , stacked_df, kernel = "linear")
#stacked_model <- randomForest(X1 ~ . , stacked_df, ntrees=1500)
stacked_model<-gbm(X1 ~ . , cv.folds = 20, interaction.depth = 10,shrinkage =0.0205,distribution='bernoulli',data=stacked_df,n.trees = 1500)
test_preds_df$X1 <- as.factor(test_preds_df$X1)
numeric_st_df_test <- sapply(test_preds_df[, !(names(test_preds_df) %in% "X1")],
                        function(x) as.numeric(as.character(x)))
predicted_stack_test <- as.data.frame(predict(pca_stack, numeric_st_df_test))[1:7]

stacked_df_test <- data.frame(predicted_stack_test, X1 = test_preds_df[, (names(test_preds_df) %in% "X1")])

preds_st_test <-  predict(stacked_model, stacked_df_test)

confusionMatrix(preds_st_test, test_datanew1$X1)

```
#STACKING WITH GLM AS TOP LAYER MODEL
```{r}
train_preds_df <- data.frame(svm = preds_svmtrain,rf =preds_train_rf,knn = preds_train_knn,tree = preds_train_rpart, tree_bag = preds_train_tree_bag,gbm = preds_train_g2, X1 = train_datanew1$X1)
# Getting all the predictions from the validation data into a dataframe.
test_preds_df<- data.frame(svm=preds_svm,rf = preds_rfvaltest,knn = preds_k,tree =preds_train_dec1, tree_bag = preds_tree_bag,gbm=preds_test_g2,X1 = test_datanew1$X1)
stack_df <- rbind(train_preds_df)

stack_df$X1 <- as.factor(stack_df$X1)
numeric_st_df <- sapply(stack_df[, !(names(stack_df) %in% "X1")],function(x) as.numeric(as.character(x)))
pca_stack <- prcomp(numeric_st_df, scale = F)
predicted_stack <- as.data.frame(predict(pca_stack, numeric_st_df))[1:7]
stacked_df <- data.frame(predicted_stack, X1 = stack_df[, (names(stack_df) %in% "X1")])
#stacked_model <- svm(X1 ~ . , stacked_df, kernel = "linear")
#stacked_model <- randomForest(X1 ~ . , stacked_df, ntrees=1500)
#stacked_model<-gbm(X1 ~ . , cv.folds = 20, interaction.depth = 10,shrinkage =0.0205,distribution='bernoulli',data=,stacked_df n.trees = 1500)
stacked_model <- glm(X1~., data = stacked_df, family = binomial,control = list(maxit=100))
test_preds_df$X1 <- as.factor(test_preds_df$X1)
numeric_st_df_test <- sapply(test_preds_df[, !(names(test_preds_df) %in% "X1")],
                        function(x) as.numeric(as.character(x)))
predicted_stack_test <- as.data.frame(predict(pca_stack, numeric_st_df_test))[1:7]

stacked_df_test <- data.frame(predicted_stack_test, X1 = test_preds_df[, (names(test_preds_df) %in% "X1")])

preds_st_test <-  predict(stacked_model, stacked_df_test,type="response")
preds_st_test <- ifelse(preds_st_test > 0.450, "1", "0")

confusionMatrix(preds_st_test, test_datanew1$X1)

```
Confusion matrix
```{r}
#logistic regression
confusionMatrix(test_data_labs,preds_test,positive = "0")
P<-precision(as.factor(preds_test),test_data_labs)
R<-recall(as.factor(preds_test),test_data_labs)
P
R
#Random forest
confusionMatrix(preds_rf, test_datanew1$X1)
P<-precision(as.factor(preds_rf),test_data_labs)
R<-recall(as.factor(preds_rf),test_data_labs)
P
R
#Random forest using cross validation.
confusionMatrix(preds_rfvaltest, test_datanew1$X1)
P<-precision(as.factor(preds_rfvaltest),test_data_labs)
R<-recall(as.factor(preds_rfvaltest),test_data_labs)
P
R
#KNN classifier
confusionMatrix(preds_knn, test_datanew1$X1)
P<-precision(as.factor(preds_knn),test_data_labs)
R<-recall(as.factor(preds_knn),test_data_labs)
P
R
#GBM
confusionMatrix(preds_gbm, test_datanew1$X1)
P<-precision(as.factor(preds_gbm),test_data_labs)
R<-recall(as.factor(preds_gbm),test_data_labs)
P
R
#GBM with tuning parameters
confusionMatrix(preds_g1, test_datanew1$X1)
P<-precision(as.factor(preds_g1),test_data_labs)
R<-recall(as.factor(preds_g1),test_data_labs)
P
R
#Decision trees with cross validation
confusionMatrix(preds_train_dec1, test_datanew1$X1)
P<-precision(as.factor(preds_train_dec1),test_data_labs)
R<-recall(as.factor(preds_train_dec1),test_data_labs)
P
R
#TAKING AVERAGE OF PREDICTIONS.
confusionMatrix(test_datanew13, test_datanew1$X1)
P<-precision(as.factor(test_datanew13),test_data_labs)
R<-recall(as.factor(test_datanew13),test_data_labs)
P
R
#TAKING MAJORITY VOTING OF PREICTIONS.
confusionMatrix(test_datanew_majority, test_datanew1$X1)
P<-precision(as.factor(test_datanew_majority),test_data_labs)
R<-recall(as.factor(test_datanew_majority),test_data_labs)
P
R
#TAKING WEIGHTED AVERAGE OF PREDICTIONS.
confusionMatrix(test_datanewaverage1, test_datanew1$X1)
P<-precision(as.factor(test_datanewaverage1),test_data_labs)
R<-recall(as.factor(test_datanewaverage1),test_data_labs)
P
R
#STACKING WITH GBM AS TOP LAYER MODEL
confusionMatrix(test_datanewstack, test_datanew1$X1)
P<-precision(as.factor(test_datanewstack),test_data_labs)
R<-recall(as.factor(test_datanewstack),test_data_labs)
P
R
#STACKING WITH GLM AS TOP LAYER MODEL
confusionMatrix(test_datanewstack15, test_datanew1$X1)
P<-precision(as.factor(test_datanewstack15),test_data_labs)
R<-recall(as.factor(test_datanewstack15),test_data_labs)
P
R



```
STACKING WITH SVM AND RANDOM FOREST ON TOP
```{r}
train_preds_df <- data.frame(svm = preds_svmtrain,rf =preds_train_rf,knn = preds_train_knn,tree = preds_train_rpart, tree_bag = preds_train_tree_bag,gbm = preds_train_g2, X1 = train_datanew1$X1)
# Getting all the predictions from the validation data into a dataframe.
test_preds_df<- data.frame(svm=preds_svm,rf = preds_rfvaltest,knn = preds_k,tree =preds_train_dec1, tree_bag = preds_tree_bag,gbm=preds_test_g2,X1 = test_datanew1$X1)
stack_df <- rbind(train_preds_df)

stack_df$X1 <- as.factor(stack_df$X1)
numeric_st_df <- sapply(stack_df[, !(names(stack_df) %in% "X1")],function(x) as.numeric(as.character(x)))
pca_stack <- prcomp(numeric_st_df, scale = F)
predicted_stack <- as.data.frame(predict(pca_stack, numeric_st_df))[1:7]
stacked_df <- data.frame(predicted_stack, X1 = stack_df[, (names(stack_df) %in% "X1")])
#stacked_model <- svm(X1 ~ . , stacked_df, kernel = "linear")
#stacked_model <- randomForest(X1 ~ . , stacked_df, ntrees=1500)
stacked_model <- randomForest(X1 ~ . , stacked_df, ntrees=1500)
test_preds_df$X1 <- as.factor(test_preds_df$X1)
numeric_st_df_test <- sapply(test_preds_df[, !(names(test_preds_df) %in% "X1")],
                        function(x) as.numeric(as.character(x)))
predicted_stack_test <- as.data.frame(predict(pca_stack, numeric_st_df_test))[1:7]

stacked_df_test <- data.frame(predicted_stack_test, X1 = test_preds_df[, (names(test_preds_df) %in% "X1")])

preds_st_test <-  predict(stacked_model, stacked_df_test)

confusionMatrix(preds_st_test, test_datanew1$X1)

```
bagging 
```{r}
library(ipred)
set.seed(1234)

model_tree_bag <- bagging(X1 ~ . , data=train_datanew1, control = rpart.control(cp = .0005, xval = 10))
preds_tree_bag <- predict(model_tree_bag, test_datanew1)
confusionMatrix(preds_tree_bag,test_datanew1$X1)
preds_train_tree_bag <- predict(model_tree_bag)

```

```{r}
xgmodel<-xgboost(data=as.matrix(train_datanew1[,-58]),label=as.numeric(train_datanew1$X1,nrounds=200))
y_pred<-predict(xgmodel,newdata=as.matrix(train_datanew1[,-58]))
table(train_datanew1$X1,)
```
C 5.0
```{r}
library(C50)
c5_tree<-C5.0(X1~.,train_datanew1,cp=1,ntrees=1500)
c5_rules<-C5.0(X1~.,train_datanew1,cp=1,ntrees=1500,rules=T)

C5imp(c5_tree,metric = "usage")
summary(c5_rules)
plot(c5_tree)
preds_c5<-predict(c5_tree,test_datanew1)
confusionMatrix(preds_c5,test_datanew1$X1)
```


